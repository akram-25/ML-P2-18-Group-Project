{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code style=\"background:#6c89cc; color:black\">Project Details</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of the project is to let students have a hands-on experience on machine \n",
    "learning application development, this will help the students to have a better \n",
    "understanding on the topics and algorithms learned. Specifically in problem formulation, \n",
    "data collection and processing, data analysis, experiment design, machine learning \n",
    "methods comparison, performance evaluation, and result analysis. Each group has its \n",
    "flexibility to choose its own problem and data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code style=\"background:#6c89cc; color:black\">Problem Background</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diseases are something that plague human beings. The top 10 causes of death in the world are all diseases, not accidents. Out of these 10, 7 of them account for diseases that are noncommunicable. These means that they are diseases that are not transmitted from one person to another. These account for 68% of the top 10 causes of death. If we frame this context at a national level, Singapore shares the same sentiment. The top 10 causes of death are also due to diseases. One such disease that we wanted to explore is stroke as it is in the top 5 causes of death worldwide and in Singappore. By taking into account the age-old idiom, \"Prevention is better than cure\", we want to delve deep into the details to figure out how to improve one's health.\n",
    "<br>\n",
    "<br>\n",
    "https://www.who.int/news-room/fact-sheets/detail/the-top-10-causes-of-death\n",
    "<br>\n",
    "https://www.moh.gov.sg/others/resources-and-statistics/principal-causes-of-death"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code style=\"background:#6c89cc; color:black\">Problem Statement</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to analyse and predict the probability of stroke based of the sourced dataset. They take into account lifestyle habits and medical history to aid with with the analysis of data as well as the training of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code style=\"background:#6c89cc; color:black\">Data Collection</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for this problem statement was sourced from Kaggle, a dataset hosting website. It consists of 11 variables, 10 of which are features and 1 of it being the target. The features variables consist of behavioural attributes as well as hereditary attributes that affect a person's probability of getting a stroke."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code style=\"background:#6c89cc; color:black\">Load the Libraries</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from flask import Flask, request, render_template\n",
    "import joblib\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.pylab import seed\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# ...and other regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code style=\"background:#6c89cc; color:black\">Load the Dataset</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the variable \"df\" to denote the instances of the dataset. \"df\" will be called in the following cells and be used for different purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Datasets/stroke_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40910, 11)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40910 entries, 0 to 40909\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   sex                40907 non-null  float64\n",
      " 1   age                40910 non-null  float64\n",
      " 2   hypertension       40910 non-null  int64  \n",
      " 3   heart_disease      40910 non-null  int64  \n",
      " 4   ever_married       40910 non-null  int64  \n",
      " 5   work_type          40910 non-null  int64  \n",
      " 6   Residence_type     40910 non-null  int64  \n",
      " 7   avg_glucose_level  40910 non-null  float64\n",
      " 8   bmi                40910 non-null  float64\n",
      " 9   smoking_status     40910 non-null  int64  \n",
      " 10  stroke             40910 non-null  int64  \n",
      "dtypes: float64(4), int64(7)\n",
      "memory usage: 3.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code style=\"background:#6c89cc; color:black\">Data Exploration</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex                  3\n",
       "age                  0\n",
       "hypertension         0\n",
       "heart_disease        0\n",
       "ever_married         0\n",
       "work_type            0\n",
       "Residence_type       0\n",
       "avg_glucose_level    0\n",
       "bmi                  0\n",
       "smoking_status       0\n",
       "stroke               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assessing for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hypertension</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart_disease</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ever_married</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work_type</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residence_type</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_glucose_level</th>\n",
       "      <td>2903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bmi</th>\n",
       "      <td>370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoking_status</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stroke</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Unique_Values\n",
       "sex                            2\n",
       "age                          111\n",
       "hypertension                   2\n",
       "heart_disease                  2\n",
       "ever_married                   2\n",
       "work_type                      5\n",
       "Residence_type                 2\n",
       "avg_glucose_level           2903\n",
       "bmi                          370\n",
       "smoking_status                 2\n",
       "stroke                         2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluating number of unique values in each column\n",
    "unique_vals=[]\n",
    "\n",
    "for col in df.columns:\n",
    "    unival=df[col].nunique()\n",
    "    unique_vals.append(unival)\n",
    "\n",
    "#Presenting the findings using a dataframe\n",
    "pd.DataFrame(unique_vals,columns=['Unique_Values'],index=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sex', 'age', 'hypertension', 'heart_disease', 'ever_married',\n",
       "       'work_type', 'Residence_type', 'avg_glucose_level', 'bmi',\n",
       "       'smoking_status', 'stroke'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['sex', 'age', 'hypertension', 'heart_disease', 'ever_married',\n",
    "       'work_type', 'Residence_type', 'avg_glucose_level', 'bmi',\n",
    "       'smoking_status', 'stroke']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code style=\"background:#6c89cc; color:black\">Feature Engineering</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to conduct feature engineering as we want to ensure that the data is well prepped to be used for the training models. It consists of handling missing values, encoding categorial variables and feature scaling to name a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we drop the columns with null values and check to ensure that they have been dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex                  0\n",
       "age                  0\n",
       "hypertension         0\n",
       "heart_disease        0\n",
       "ever_married         0\n",
       "work_type            0\n",
       "Residence_type       0\n",
       "avg_glucose_level    0\n",
       "bmi                  0\n",
       "smoking_status       0\n",
       "stroke               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assessing for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40907, 11)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code style=\"background:#6c89cc; color:black\">Creating Models</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create the X and Y arrays. These will then further be split into two more arrays, with each having a \"train\" and \"test\" set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['sex', 'age', 'hypertension', 'heart_disease', 'ever_married',\n",
    "       'work_type', 'Residence_type', 'avg_glucose_level', 'bmi',\n",
    "       'smoking_status']].values\n",
    "y = df['stroke'].values\n",
    "\n",
    "accuracies = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties of train/test datasets\n",
      "Train dataset length: 27407\n",
      "First element of train dataset: (tensor([-1.1240,  1.0927, -0.5221, -0.3813,  0.4644,  0.6891,  0.9688, -0.7199,\n",
      "         0.0705, -0.9752]), tensor(0))\n",
      "Test dataset length: 13500\n",
      "First element of test dataset: (tensor([-1.1240, -0.9938, -0.5221, -0.3813, -2.1531, -1.8677, -1.0322, -0.0179,\n",
      "        -0.7185,  1.0255]), tensor(0))\n",
      "\n",
      " Properties of loaders\n",
      "Number of batches in train loader: 3426\n",
      "Batch size in train loader: 8\n",
      "Number of batches in test loader: 1688\n",
      "Batch size in test loader: 8\n"
     ]
    }
   ],
   "source": [
    "# Properties of train and test datasets\n",
    "print(\"Properties of train/test datasets\")\n",
    "print(\"Train dataset length:\", len(train_dataset))\n",
    "print(\"First element of train dataset:\", train_dataset[0])\n",
    "print(\"Test dataset length:\", len(test_dataset))\n",
    "print(\"First element of test dataset:\", test_dataset[0])\n",
    "\n",
    "# Properties of train and test loader\n",
    "print(\"\\n Properties of loaders\")\n",
    "print(\"Number of batches in train loader:\", len(train_loader))\n",
    "print(\"Batch size in train loader:\", train_loader.batch_size)\n",
    "print(\"Number of batches in test loader:\", len(test_loader))\n",
    "print(\"Batch size in test loader:\", test_loader.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BinaryClassificationMLP.__init__() missing 1 required positional argument: 'input_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Create a new model with correct input size\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBinaryClassificationMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mTypeError\u001b[0m: BinaryClassificationMLP.__init__() missing 1 required positional argument: 'input_size'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define model with proper input/output dimensions\n",
    "class BinaryClassificationMLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(BinaryClassificationMLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()  # Outputs probability between 0-1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Use binary cross-entropy loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create a new model with correct input size\n",
    "model = BinaryClassificationMLP().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties of optimizer and criterion:\n",
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Criterion: CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Properties of optimizer and criterion\n",
    "print(\"Properties of optimizer and criterion:\")\n",
    "print(\"Optimizer:\", optimizer)\n",
    "print(\"Criterion:\", criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch shape check:\n",
      "Input shape: torch.Size([8, 10]), Labels shape: torch.Size([8])\n",
      "Epoch 0, Loss: 0.0000\n",
      "Epoch 20, Loss: 0.0000\n",
      "Epoch 40, Loss: 0.0000\n",
      "Epoch 60, Loss: 0.0000\n",
      "Epoch 80, Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 100\n",
    "logs = []\n",
    "\n",
    "# Enable debug info to check shapes\n",
    "print(f\"First batch shape check:\")\n",
    "for inputs, labels in train_loader:\n",
    "    print(f\"Input shape: {inputs.shape}, Labels shape: {labels.shape}\")\n",
    "    break\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)  # Make sure model is on the right device\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Move tensors to the correct device and ensure correct types\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device).float().view(-1, 1)  # Reshape labels for regression task\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)  # Compute model predictions\n",
    "        \n",
    "        # Make sure output and label dimensions match\n",
    "        if outputs.shape != labels.shape:\n",
    "            outputs = outputs.view(labels.shape)\n",
    "            \n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update model parameters\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        # Log progress every 20 epochs\n",
    "        logs.append(f\"Epoch {epoch}, Loss: {epoch_loss:.4f}\")\n",
    "        print(f\"Epoch {epoch}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 49.96%\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Move tensors to device and ensure correct formats\n",
    "        inputs = inputs.to(device).float()\n",
    "        labels = labels.to(device)  # For classification, keep as integers\n",
    "        \n",
    "        # Set the forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # For regression tasks, compare if predictions are close to targets\n",
    "        # For classification tasks, get the predicted class\n",
    "        if outputs.shape != labels.shape:\n",
    "            if outputs.dim() > 1 and outputs.size(1) > 1:\n",
    "                # Classification task with multiple output classes\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            else:\n",
    "                # Regression task\n",
    "                outputs = outputs.view(-1)\n",
    "                predicted = outputs.round()  # Round to nearest integer for evaluation\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        else:\n",
    "            # Direct comparison\n",
    "            predicted = outputs.round()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "        # Get the number of observations in the batch\n",
    "        total += labels.size(0)\n",
    "\n",
    "mlp_accuracy = correct / total\n",
    "print(f'Test Accuracy: {mlp_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
